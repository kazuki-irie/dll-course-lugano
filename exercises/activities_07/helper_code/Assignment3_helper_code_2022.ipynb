{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JILVek0PhzbT"
      },
      "source": [
        "# Helper Code for Assignment 3 (RNN language models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxdOkx_htjZ"
      },
      "source": [
        "## Reading raw text file & Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EawFomcOh3XE"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A5DmY30hHHP"
      },
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "        \n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "        \n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1        \n",
        "        \n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "\n",
        "# Read the raw txt file and generate a 1D PyTorch tensor\n",
        "# containing the whole text mapped to sequence of token IDs, and a vocab object.\n",
        "class TextData:\n",
        "\n",
        "    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):\n",
        "        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def text_to_data(self, text_file, vocab, extend_vocab, device):\n",
        "        \"\"\"Read a raw text file and create its tensor and the vocab.\n",
        "\n",
        "        Args:\n",
        "          text_file: a path to a raw text file.\n",
        "          vocab: a Vocab object\n",
        "          extend_vocab: bool, if True extend the vocab\n",
        "          device: device\n",
        "\n",
        "        Returns:\n",
        "          Tensor representing the input text, vocab file\n",
        "\n",
        "        \"\"\"\n",
        "        assert os.path.exists(text_file)\n",
        "        if vocab is None:\n",
        "            vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "\n",
        "        # Construct data\n",
        "        full_text = []\n",
        "        print(f\"Reading text file from: {text_file}\")\n",
        "        with open(text_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                for token in tokens:\n",
        "                    # get index will extend the vocab if the input\n",
        "                    # token is not yet part of the text.\n",
        "                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "\n",
        "        # convert to tensor\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data, vocab\n",
        "    \n",
        "\n",
        "# Since there is no need for schuffling the data, we just have to split\n",
        "# the text data according to the batch size and bptt length.\n",
        "# The input to be fed to the model will be batch[:-1]\n",
        "# The target to be used for the loss will be batch[1:]\n",
        "class DataBatches:\n",
        "\n",
        "    def __init__(self, data, bsz, bptt_len, pad_id):\n",
        "        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "    def create_batch(self, input_data, bsz, bptt_len, pad_id):\n",
        "        \"\"\"Create batches from a TextData object .\n",
        "\n",
        "        Args:\n",
        "          input_data: a TextData object.\n",
        "          bsz: int, batch size\n",
        "          bptt_len: int, bptt length\n",
        "          pad_id: int, ID of the padding token\n",
        "\n",
        "        Returns:\n",
        "          List of tensors representing batches\n",
        "\n",
        "        \"\"\"\n",
        "        batches = []  # each element in `batches` is (len, B) tensor\n",
        "        text_len = len(input_data)\n",
        "        segment_len = text_len // bsz + 1\n",
        "\n",
        "        # Question: Explain the next two lines!\n",
        "        padded = input_data.data.new_full((segment_len * bsz,), pad_id)\n",
        "        padded[:text_len] = input_data.data\n",
        "        padded = padded.view(bsz, segment_len).t()\n",
        "        num_batches = segment_len // bptt_len + 1\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # Prepare batches such that the last symbol of the current batch\n",
        "            # is the first symbol of the next batch.\n",
        "            if i == 0:\n",
        "                # Append a dummy start symbol using pad token\n",
        "                batch = torch.cat(\n",
        "                    [padded.new_full((1, bsz), pad_id),\n",
        "                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)\n",
        "                batches.append(batch)\n",
        "            else:\n",
        "                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])\n",
        "\n",
        "        return batches"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u-9Q7K4hVbL",
        "outputId": "6ef1276b-82c7-4c0d-8545-7d3cae9bfe68"
      },
      "source": [
        "# downlaod the text\n",
        "# Make sure to go to the link and check how the text looks like.\n",
        "!wget http://www.gutenberg.org/files/49010/49010-0.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-09 21:48:39--  http://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/49010/49010-0.txt [following]\n",
            "--2022-11-09 21:48:40--  https://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185303 (181K) [text/plain]\n",
            "Saving to: ‘49010-0.txt.3’\n",
            "\n",
            "49010-0.txt.3       100%[===================>] 180.96K   259KB/s    in 0.7s    \n",
            "\n",
            "2022-11-09 21:48:42 (259 KB/s) - ‘49010-0.txt.3’ saved [185303/185303]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRqPHT_zhgNa"
      },
      "source": [
        "# This is for Colab. Adapt the path if needed.\n",
        "text_path = \"/content/49010-0.txt\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hff1FxeohHHR",
        "outputId": "b037782e-4dd5-4af3-c464-43b70593ad16"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = TextData(text_path, device=DEVICE)\n",
        "batches = DataBatches(my_data, batch_size, bptt_len, pad_id=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file from: /content/49010-0.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Tq3WITXfTc67"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxszGttKhHHU"
      },
      "source": [
        "# RNN based language model\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hidden_dim, num_layers):\n",
        "        \"\"\"Parameters:\n",
        "        \n",
        "          num_classes (int): number of input/output classes\n",
        "          emb_dim (int): token embedding size\n",
        "          hidden_dim (int): hidden layer size of RNNs\n",
        "          num_layers (int): number of RNN layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.input_layer = nn.Embedding(num_classes, emb_dim)\n",
        "        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers)\n",
        "        self.out_layer = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        emb = self.input_layer(input)\n",
        "        output, state = self.rnn(emb, state)\n",
        "        output = self.out_layer(output)\n",
        "        output = output.view(-1, self.num_classes)\n",
        "        return output, state\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)\n",
        "\n",
        "\n",
        "# To be modified for LSTM...\n",
        "def custom_detach(h):\n",
        "    return h.detach()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding"
      ],
      "metadata": {
        "id": "FyW5_JUXTgAV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA5DRnBAhHHV"
      },
      "source": [
        "@torch.no_grad()\n",
        "def complete(model, prompt, steps, sample=False):\n",
        "    \"\"\"Complete the prompt for as long as given steps using the model.\n",
        "    \n",
        "    Parameters:\n",
        "      model: language model\n",
        "      prompt (str): text segment to be completed\n",
        "      steps (int): number of decoding steps.\n",
        "      sample (bool): If True, sample from the model. Otherwise greedy.\n",
        "\n",
        "    Returns:\n",
        "      completed text (str)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    out_list = []\n",
        "    \n",
        "    # forward the prompt, compute prompt's ppl\n",
        "    prompt_list = []\n",
        "    char_prompt = list(prompt)\n",
        "    for char in char_prompt:\n",
        "        prompt_list.append(my_data.vocab.string_to_id[char])\n",
        "    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)\n",
        "    \n",
        "    states = model.init_hidden(1)\n",
        "    logits, states = model(x, states)\n",
        "    probs = F.softmax(logits[-1], dim=-1)\n",
        "        \n",
        "    if sample:\n",
        "        assert False, \"Implement me!\"\n",
        "    else:\n",
        "        max_p, ix = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "    out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "    x = ix.unsqueeze(1)\n",
        "    \n",
        "    # decode \n",
        "    for k in range(steps):\n",
        "        logits, states = model(x, states)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        if sample:  # sample from the distribution or take the most likely\n",
        "            assert False, \"Implement me!\"\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix\n",
        "    return ''.join(out_list)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaVkFEz8hHHV",
        "outputId": "27eb528b-52cb-4add-a963-4b712db9d25f"
      },
      "source": [
        "learning_rate = 0.0005\n",
        "clipping = 1.0\n",
        "embedding_size = 64\n",
        "rnn_size = 2048\n",
        "rnn_num_layers = 1\n",
        "\n",
        "# vocab_size = len(module.vocab.itos)\n",
        "vocab_size = len(my_data.vocab.id_to_string)\n",
        "print(F\"vocab size: {vocab_size}\")\n",
        "\n",
        "model = RNNModel(\n",
        "    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=rnn_size,\n",
        "    num_layers=rnn_num_layers)\n",
        "model = model.to(DEVICE)\n",
        "hidden = model.init_hidden(batch_size)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "-Bhimbs4T0dn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xTOfBUIhHHW",
        "outputId": "13b9f762-6b88-45c1-fd16-cc35d7fd93b6"
      },
      "source": [
        "# Training\n",
        "\n",
        "num_epochs = 30\n",
        "report_every = 30\n",
        "prompt = \"Dogs like best to\"\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    print(f\"=== start epoch {ep} ===\")\n",
        "    state = model.init_hidden(batch_size)\n",
        "    for idx in range(len(batches)):\n",
        "        batch = batches[idx]\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        state = custom_detach(state)\n",
        "        \n",
        "        input = batch[:-1]\n",
        "        target = batch[1:].reshape(-1)\n",
        "\n",
        "        bsz = input.shape[1]\n",
        "        prev_bsz = state.shape[1]\n",
        "        if bsz != prev_bsz:\n",
        "            state = state[:, :bsz, :]\n",
        "        output, state = model(input, state)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "        if idx % report_every == 0:\n",
        "            print(f\"train loss: {loss.item()}\")  # replace me by the line below!\n",
        "            # print(f\"train ppl: {}\")\n",
        "            generated_text = complete(model, prompt, 128, sample=False)\n",
        "            print(f'----------------- epoch/batch {ep}/{idx} -----------------')\n",
        "            print(prompt)\n",
        "            print(generated_text)\n",
        "            print(f'----------------- end generated text -------------------')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== start epoch 0 ===\n",
            "train loss: 4.6876397132873535\n",
            "----------------- epoch/batch 0/0 -----------------\n",
            "Dogs like best to\n",
            "                                                                                                                                 \n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.6431944370269775\n",
            "----------------- epoch/batch 0/30 -----------------\n",
            "Dogs like best to\n",
            "  an  he  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou  ou \n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.4694268703460693\n",
            "----------------- epoch/batch 0/60 -----------------\n",
            "Dogs like best to\n",
            " the  an the  and and the  and and the  and and the  and and the  and and the  and and the  and and the  and and the  and and the\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 1 ===\n",
            "train loss: 2.321481943130493\n",
            "----------------- epoch/batch 1/0 -----------------\n",
            "Dogs like best to\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.2129366397857666\n",
            "----------------- epoch/batch 1/30 -----------------\n",
            "Dogs like best to\n",
            " the wish the pand the pand the pand the pand the pand the pand the pand the pand the pand the pand the pand the pand the pand th\n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.2108154296875\n",
            "----------------- epoch/batch 1/60 -----------------\n",
            "Dogs like best to\n",
            " the sall the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore the sore th\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 2 ===\n",
            "train loss: 2.0848746299743652\n",
            "----------------- epoch/batch 2/0 -----------------\n",
            "Dogs like best to\n",
            " the sain the the sain the Fox the sain the Fox the sain the Fox the sain the Fox the sain the Fox the sain the Fox the sain the \n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.0273866653442383\n",
            "----------------- epoch/batch 2/30 -----------------\n",
            "Dogs like best to\n",
            " the will be and the Guther and the Guther and the Guther and the Guther and the Guther and the Guther and the Guther and the Gut\n",
            "----------------- end generated text -------------------\n",
            "train loss: 2.039806842803955\n",
            "----------------- epoch/batch 2/60 -----------------\n",
            "Dogs like best to\n",
            " the said the fored the sered the fored the sered the fored the sered the fored the sered the fored the sered the fored the sered\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 3 ===\n",
            "train loss: 1.919404149055481\n",
            "----------------- epoch/batch 3/0 -----------------\n",
            "Dogs like best to\n",
            " the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to th\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.8992729187011719\n",
            "----------------- epoch/batch 3/30 -----------------\n",
            "Dogs like best to\n",
            " see the seave the said the Gras he was the said the Gras he was the said the Gras he was the said the Gras he was the said the G\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.9182299375534058\n",
            "----------------- epoch/batch 3/60 -----------------\n",
            "Dogs like best to\n",
            " see the mant the mand the mand the mand the mand the mand the mand the mand the mand the mand the mand the mand the mand the man\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 4 ===\n",
            "train loss: 1.7943700551986694\n",
            "----------------- epoch/batch 4/0 -----------------\n",
            "Dogs like best to\n",
            " the bean a paid the Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.796945333480835\n",
            "----------------- epoch/batch 4/30 -----------------\n",
            "Dogs like best to\n",
            " see his began to see his began to see his began to see his began to see his began to see his began to see his began to see his b\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.8004215955734253\n",
            "----------------- epoch/batch 4/60 -----------------\n",
            "Dogs like best to\n",
            " be the hand the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 5 ===\n",
            "train loss: 1.6692856550216675\n",
            "----------------- epoch/batch 5/0 -----------------\n",
            "Dogs like best to\n",
            " the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.689051866531372\n",
            "----------------- epoch/batch 5/30 -----------------\n",
            "Dogs like best to\n",
            " the tree the donser the down and the Gutere the tree the donser the down and the Gutere the tree the donser the down and the Gut\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.6991289854049683\n",
            "----------------- epoch/batch 5/60 -----------------\n",
            "Dogs like best to\n",
            " the forther with the forther with the forther with the forther with the forther with the forther with the forther with the forth\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 6 ===\n",
            "train loss: 1.5783113241195679\n",
            "----------------- epoch/batch 6/0 -----------------\n",
            "Dogs like best to\n",
            " the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.6076958179473877\n",
            "----------------- epoch/batch 6/30 -----------------\n",
            "Dogs like best to\n",
            " the did the could not be and great his beart the strong and get his his his his his his his his his his his his his his his his \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.6107088327407837\n",
            "----------------- epoch/batch 6/60 -----------------\n",
            "Dogs like best to\n",
            " such a wall have the man and the Wolf stopped the man and the Wolf stopped the man and the Wolf stopped the man and the Wolf sto\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 7 ===\n",
            "train loss: 1.5060406923294067\n",
            "----------------- epoch/batch 7/0 -----------------\n",
            "Dogs like best to\n",
            " the round of the work or a stare a bear a stare a bear a stare a bear a stare a bear a stare a bear a stare a bear a stare a bea\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.5263310670852661\n",
            "----------------- epoch/batch 7/30 -----------------\n",
            "Dogs like best to\n",
            " the tree the down and the Grasshoppers the serves. The Wolf and the Grasshoppers the serves. The Wolf and the Grasshoppers the s\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.5387431383132935\n",
            "----------------- epoch/batch 7/60 -----------------\n",
            "Dogs like best to\n",
            " said the country which the mart and the Fox sad the man had been the man had been the man had been the man had been the man had \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 8 ===\n",
            "train loss: 1.4530649185180664\n",
            "----------------- epoch/batch 8/0 -----------------\n",
            "Dogs like best to\n",
            " the don’s so the country and the Stare was a stare a beast but the Fox and the Stare was a stare a beast but the Fox and the Sta\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.467328667640686\n",
            "----------------- epoch/batch 8/30 -----------------\n",
            "Dogs like best to\n",
            " her and see the tog the tree the donser to get him an the dispession of the forest for fell and see the tog the tree the donser \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.4463286399841309\n",
            "----------------- epoch/batch 8/60 -----------------\n",
            "Dogs like best to\n",
            " the partingly and brinked to the parting to the partingly and brinked to the parting to the partingly and brinked to the parting\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 9 ===\n",
            "train loss: 1.3891172409057617\n",
            "----------------- epoch/batch 9/0 -----------------\n",
            "Dogs like best to\n",
            " the money and the Stork                                                                                                         \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.4060884714126587\n",
            "----------------- epoch/batch 9/30 -----------------\n",
            "Dogs like best to\n",
            " pass the and stand on the donsered the Lion said the Gull on a strong and stand on the donsered the Lion said the Gull on a stro\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.3728930950164795\n",
            "----------------- epoch/batch 9/60 -----------------\n",
            "Dogs like best to\n",
            " the present them and said, “The mest the seeved the hard forth in the present them and said, “The mest the seeved the hard forth\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 10 ===\n",
            "train loss: 1.3444100618362427\n",
            "----------------- epoch/batch 10/0 -----------------\n",
            "Dogs like best to\n",
            " the most pards when he was not a bear the bead wetting the water is the water to the most pards when he was not a bear the bead \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.3563950061798096\n",
            "----------------- epoch/batch 10/30 -----------------\n",
            "Dogs like best to\n",
            " his and said, “I will try to pear as to the distributing and stand and grane a little promoting and grane across to a part, was \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.29282808303833\n",
            "----------------- epoch/batch 10/60 -----------------\n",
            "Dogs like best to\n",
            " the present them and said: “Oh, I dive the tree from to the present them and said: “Oh, I dive the tree from to the present them\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 11 ===\n",
            "train loss: 1.2783935070037842\n",
            "----------------- epoch/batch 11/0 -----------------\n",
            "Dogs like best to\n",
            " the most pards and brouding the ment and began to be it is the way is the way is the way is the way is the way is the way is the\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.3041355609893799\n",
            "----------------- epoch/batch 11/30 -----------------\n",
            "Dogs like best to\n",
            " the top of the work, who will be as well as he was not berat him and reach that where you will be as well as he was not berat hi\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.2260369062423706\n",
            "----------------- epoch/batch 11/60 -----------------\n",
            "Dogs like best to\n",
            " the provide the great replied: “What a forest the feet shall he had not mose for the provident them a King, Jupiter was the coun\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 12 ===\n",
            "train loss: 1.228304147720337\n",
            "----------------- epoch/batch 12/0 -----------------\n",
            "Dogs like best to\n",
            " the edel the beasts to pard it the mounte to pard us to the mound, and along the beasts to pard it the mounte to pard us to the \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.245198369026184\n",
            "----------------- epoch/batch 12/30 -----------------\n",
            "Dogs like best to\n",
            " him, better than the some made him to be a little on the some mons, we must be so not come to the top of the work that you will \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.179917335510254\n",
            "----------------- epoch/batch 12/60 -----------------\n",
            "Dogs like best to\n",
            " the provide a siggle of the food so such as the way he could she had the great sport that it was she started the hard should be \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 13 ===\n",
            "train loss: 1.172319769859314\n",
            "----------------- epoch/batch 13/0 -----------------\n",
            "Dogs like best to\n",
            " the greatest of the would have all the belless of the would have all the belless of the would have all the belless of the would \n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.177200436592102\n",
            "----------------- epoch/batch 13/30 -----------------\n",
            "Dogs like best to\n",
            " his lost of it is they comply with all his most for their pains were and the How eation of the little snow when they heard the s\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.119472861289978\n",
            "----------------- epoch/batch 13/60 -----------------\n",
            "Dogs like best to\n",
            " the present that was the care and said to his master, such as the way his great many and was stream. “If what it was hard forth \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 14 ===\n",
            "train loss: 1.1094472408294678\n",
            "----------------- epoch/batch 14/0 -----------------\n",
            "Dogs like best to\n",
            " the great sportance of any with any beant all the and began to the greatest of a stook it the man and the Fox were a beant by st\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.125437617301941\n",
            "----------------- epoch/batch 14/30 -----------------\n",
            "Dogs like best to\n",
            " pass becoment of the down below. They said, “and the Horse went of it. The sering for their pains were once contended the some s\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.0804295539855957\n",
            "----------------- epoch/batch 14/60 -----------------\n",
            "Dogs like best to\n",
            " are to come in his great proceeded to the servine to her monter,” replied the hole. When he had lost nearly fell in the way hers\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 15 ===\n",
            "train loss: 1.0259977579116821\n",
            "----------------- epoch/batch 15/0 -----------------\n",
            "Dogs like best to\n",
            " the tree. “son’t yourse was a stories of the ask, was a moment.\n",
            "\n",
            "“Than a moment.\n",
            "\n",
            "“Than a moment.\n",
            "\n",
            "“Than a moment.\n",
            "\n",
            "“Than a mome\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.070488452911377\n",
            "----------------- epoch/batch 15/30 -----------------\n",
            "Dogs like best to\n",
            " cate near which he had the greater pare of the work but soon the distunce of the world on the dispused to the dogs. They seen by\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.019368290901184\n",
            "----------------- epoch/batch 15/60 -----------------\n",
            "Dogs like best to\n",
            " adistributing of the tree, he said, “Ond in all his might be partinul such and his own presently with the preasure in the world \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 16 ===\n",
            "train loss: 0.9668136239051819\n",
            "----------------- epoch/batch 16/0 -----------------\n",
            "Dogs like best to\n",
            " the Fox dest them a provide a defect in pard is a proved a proverd and provide a defr a hungry and asked the Frog from the punfo\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.0349862575531006\n",
            "----------------- epoch/batch 16/30 -----------------\n",
            "Dogs like best to\n",
            " a good price of a Hew look doing so that there were on which was so\n",
            "nich sech a bear we all the she had to a Hare (Gan ind the H\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9415680766105652\n",
            "----------------- epoch/batch 16/60 -----------------\n",
            "Dogs like best to\n",
            " accorned to any one of the present to see where the master was the man held his cloak, saching to the proud of the creature for \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 17 ===\n",
            "train loss: 0.9287863373756409\n",
            "----------------- epoch/batch 17/0 -----------------\n",
            "Dogs like best to\n",
            " the world have looked at the poor man to the ponness of his thing that he was lost as not a Fox, who had no one of the world hav\n",
            "----------------- end generated text -------------------\n",
            "train loss: 1.03075110912323\n",
            "----------------- epoch/batch 17/30 -----------------\n",
            "Dogs like best to\n",
            " all, be careful to fees to asted by to take a look and was an agreed to dear friends with a load with all the same, and the Wolf\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9453029632568359\n",
            "----------------- epoch/batch 17/60 -----------------\n",
            "Dogs like best to\n",
            " Jupiter was so grade and so\n",
            "bug that it was not a bird.”\n",
            "\n",
            "“What as to look from the service. Well, threw that can be strated by \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 18 ===\n",
            "train loss: 0.8722571134567261\n",
            "----------------- epoch/batch 18/0 -----------------\n",
            "Dogs like best to\n",
            " hear my, he than all the asine where the acto favoring to the little Fox, who had to go shall be for a while, and said: “Dhan al\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9877809286117554\n",
            "----------------- epoch/batch 18/30 -----------------\n",
            "Dogs like best to\n",
            " a fine, sun to the Lither, and the Har so all the Lion, and the Hares said the Wolf, and the Wolf was grasside arade arried and \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9019822478294373\n",
            "----------------- epoch/batch 18/60 -----------------\n",
            "Dogs like best to\n",
            " the little Fish                                                                                                                 \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 19 ===\n",
            "train loss: 0.8278016448020935\n",
            "----------------- epoch/batch 19/0 -----------------\n",
            "Dogs like best to\n",
            " it that the\n",
            "mounder the peeples                                                                                                 \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9750276207923889\n",
            "----------------- epoch/batch 19/30 -----------------\n",
            "Dogs like best to\n",
            " his mind, so that your very with all his might of something to the both. You are the same,” replied the Wolf, and your sen that \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.8227693438529968\n",
            "----------------- epoch/batch 19/60 -----------------\n",
            "Dogs like best to\n",
            " heard that he found that she was exery and trasshapped a buttery of the person of the creature, and this time the stream.\n",
            "\n",
            "The W\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 20 ===\n",
            "train loss: 0.8146244287490845\n",
            "----------------- epoch/batch 20/0 -----------------\n",
            "Dogs like best to\n",
            " Jupiter and the Fox                                                                                                             \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9451459646224976\n",
            "----------------- epoch/batch 20/30 -----------------\n",
            "Dogs like best to\n",
            " be a ring, and discontination, and the Wolf took up an that you have one more dance to be at them and yin will not he had the gr\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7988684177398682\n",
            "----------------- epoch/batch 20/60 -----------------\n",
            "Dogs like best to\n",
            " ask for a gut of this well snown that no one good to help he was thisself outside.”\n",
            "\n",
            "The Woodmand a great state as the water, an\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 21 ===\n",
            "train loss: 0.7928167581558228\n",
            "----------------- epoch/batch 21/0 -----------------\n",
            "Dogs like best to\n",
            " Jupiter and the Fox, “to said the Frog was dragged out of the cock, who was eating his fortain it make a look at the trunk, down\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.9350995421409607\n",
            "----------------- epoch/batch 21/30 -----------------\n",
            "Dogs like best to\n",
            " a pitcher was the could got for the little one works in for little of the world of his pail, the Boar, a wight wet in the hand, \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7672075033187866\n",
            "----------------- epoch/batch 21/60 -----------------\n",
            "Dogs like best to\n",
            " grant that the Fox could not be faurthing for surply and kindly to be found in the person of the cullantly asking in the water.\n",
            "\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 22 ===\n",
            "train loss: 0.7489804625511169\n",
            "----------------- epoch/batch 22/0 -----------------\n",
            "Dogs like best to\n",
            " Jupiter and the Frog frisked about down below.\n",
            "\n",
            "The Foxes at a Fion, If I were truen,” said the Frog, as he to his home. The rem\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.8744738101959229\n",
            "----------------- epoch/batch 22/30 -----------------\n",
            "Dogs like best to\n",
            " see the top scarched the holest of has to walk by his caught be said to his failt that he was the act the tim the strencted that\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7734004855155945\n",
            "----------------- epoch/batch 22/60 -----------------\n",
            "Dogs like best to\n",
            " the poor Man, he found that he could not be suffer for our own\n",
            "forest.\n",
            "\n",
            "“The full Project Gutenberg-tm electronic work in the Un\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 23 ===\n",
            "train loss: 0.7046681046485901\n",
            "----------------- epoch/batch 23/0 -----------------\n",
            "Dogs like best to\n",
            " Jupiter and the reses as he render than the next time the Frog frisked about a family. The well, and a good donations to the req\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.8076218366622925\n",
            "----------------- epoch/batch 23/30 -----------------\n",
            "Dogs like best to\n",
            " tree that caught nothing to drink for him to his long exos the poor collection will replied the strangled outs sed the scropped \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7163645625114441\n",
            "----------------- epoch/batch 23/60 -----------------\n",
            "Dogs like best to\n",
            " the pull came up, deep with stones, home to the ground in the country where you\n",
            "                                                \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 24 ===\n",
            "train loss: 0.6877094507217407\n",
            "----------------- epoch/batch 24/0 -----------------\n",
            "Dogs like best to\n",
            " the tree, “Will you, my dear, from teres. I\n",
            "\n",
            "The Hawk and the Stage the next time the Frog from the pond said, “Sirs, I differ y\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7673252820968628\n",
            "----------------- epoch/batch 24/30 -----------------\n",
            "Dogs like best to\n",
            " tell the same,”\n",
            "ans been the texts when he had the good for the little spions. The see the tex to until the lioned and see the t\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.6972619891166687\n",
            "----------------- epoch/batch 24/60 -----------------\n",
            "Dogs like best to\n",
            " the field, he speaded him worts the Frogs who asked his while, and so preadidg the water, heard him.\n",
            "\n",
            "Ty bland at the perpon wit\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 25 ===\n",
            "train loss: 0.6650915741920471\n",
            "----------------- epoch/batch 25/0 -----------------\n",
            "Dogs like best to\n",
            " the tree, and tried to five send friends.\n",
            "\n",
            "The little Fibles for much mere as down and set for himself, “I can stop times fright\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7532520890235901\n",
            "----------------- epoch/batch 25/30 -----------------\n",
            "Dogs like best to\n",
            " play we highton, which has things, that no matted to set her so he found to the food some one to additions of the full Project G\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.6709044575691223\n",
            "----------------- epoch/batch 25/60 -----------------\n",
            "Dogs like best to\n",
            " the person should be taught to should be taught to should be taught to should be taught to should be taught to should be taught \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 26 ===\n",
            "train loss: 0.6259636878967285\n",
            "----------------- epoch/batch 26/0 -----------------\n",
            "Dogs like best to\n",
            " the tree, and try to be friends in serve with me.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE MISCE wTOR UND THE FULLER\n",
            "\n",
            "\n",
            "A CRUIC is a beliest fertured her neck.”\n",
            "\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7294304370880127\n",
            "----------------- epoch/batch 26/30 -----------------\n",
            "Dogs like best to\n",
            " ald tor upited rame bray.”\n",
            "\n",
            "“Oh, not come to the Lamb, was in the United States, you'll have it is the strong.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE MOUSE, T\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.6487226486206055\n",
            "----------------- epoch/batch 26/60 -----------------\n",
            "Dogs like best to\n",
            " the copyright law meat it happen that he could find now.”\n",
            "\n",
            "As the longer forms of the man in the United States. Compliance requi\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 27 ===\n",
            "train loss: 0.5896076560020447\n",
            "----------------- epoch/batch 27/0 -----------------\n",
            "Dogs like best to\n",
            "\n",
            "himself, you have lost my neid\n",
            "bogh can the mank of mortack in this were merthed it both.”\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE HUSBANDMAN pi\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.7005913257598877\n",
            "----------------- epoch/batch 27/30 -----------------\n",
            "Dogs like best to\n",
            " additions or the Shesherd, and that is all the stranger. She gave him the struatife of the work as before, that you were all the\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.6010943055152893\n",
            "----------------- epoch/batch 27/60 -----------------\n",
            "Dogs like best to\n",
            " a for a good precentation is a good all the best from the same good for himself, “How not a theng. So\n",
            "so that the Ant caught him\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 28 ===\n",
            "train loss: 0.5774997472763062\n",
            "----------------- epoch/batch 28/0 -----------------\n",
            "Dogs like best to\n",
            " help heree to\n",
            "sale persions, and seeing him with him.\n",
            "\n",
            "“Oh, how could be do a dened the latter\n",
            "                                 \n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.6584056615829468\n",
            "----------------- epoch/batch 28/30 -----------------\n",
            "Dogs like best to\n",
            " true volled in the water so that I cannot drink it?”\n",
            "\n",
            "The Lamb, the Wolf did not like conarching as the frogs who asked the chee\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.5772512555122375\n",
            "----------------- epoch/batch 28/60 -----------------\n",
            "Dogs like best to\n",
            " eat it along the head Frog, and swallowed it. Sooking except for the least Finesped to make the farge and the Ond alooks, hear, \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 29 ===\n",
            "train loss: 0.556694746017456\n",
            "----------------- epoch/batch 29/0 -----------------\n",
            "Dogs like best to\n",
            " help hereen performed by the providing a part of the work electronic work, or any pard once deppered and\n",
            "came and strunger, “por\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.596777617931366\n",
            "----------------- epoch/batch 29/30 -----------------\n",
            "Dogs like best to\n",
            " protect his too father while he was the one\n",
            "then the Dog                 115\n",
            "    The Dog and the Lion                  114\n",
            "    T\n",
            "----------------- end generated text -------------------\n",
            "train loss: 0.5649505853652954\n",
            "----------------- epoch/batch 29/60 -----------------\n",
            "Dogs like best to\n",
            " the same good for food for\n",
            "humpleasy manning in the penson of it. The Fox and the Seas should de in the life.\n",
            "\n",
            "In mint read brin\n",
            "----------------- end generated text -------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxKKDv4ohHHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "242459f9-5f4f-4f81-fdc8-c545b7ce93e1"
      },
      "source": [
        "complete(model, \"THE DONKEY IN THE LION’S SKIN\", 512, sample=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nA DONKEY once passer the Frogs, “I will not go downing a way, without more earles that you said to his mouse,\\nand so not see, go donate.\\n\\n\\n\\n\\nTHE BULL AND THE CAMEL\\n\\n\\nA WOLF was once strong sticks to come and see dear fellow,” said she, “we say down the sold too late the father said the Fox.\\n\\n“No,” replied the Fox.\\n\\n“Nood, and began to called the Fox.\\n\\n“No,” said the Fox.\\n\\n“Nood, and began to called the Fox.\\n\\n“No,” said the Fox.\\n\\n“Nood, and began to called the Fox.\\n\\n“No,” said the Fox.\\n\\n“Nood, and began to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}